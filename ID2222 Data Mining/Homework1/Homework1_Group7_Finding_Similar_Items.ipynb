{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Data - Amazon Movie Reviews - 9.33 GB >> a few sub documents can be generated\n",
    "# /Users/melih/Documents/Masters/KTH_DASE-ICT Innovation_Data Science/7.Data Mining/Project/test_graph/KTH-ID2211-Data-Mining-master/data/movies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shingling:\n",
    "    def Compute_Shingle_Set_string(self, input_string, k_shingles = 10, run_mode = 'normal'):\n",
    "        total_characters = len(input_string)\n",
    "        character_processing = 0\n",
    "        shingle_set = list()\n",
    "        shingle_set_hash = list()\n",
    "        \n",
    "        # SETs are unordered,that's why we do our own check for the uniqueness\n",
    "        #https://docs.python.org/2/library/sets.html\n",
    "        while (character_processing + k_shingles) < total_characters + 1:            \n",
    "            shingle = input_string[character_processing: character_processing + k_shingles]\n",
    "            #print(shingle)\n",
    "            if shingle not in shingle_set:\n",
    "                shingle_set.append(shingle)\n",
    "                shingle_set_hash.append(hash(shingle))\n",
    "            character_processing += 1\n",
    "            \n",
    "        if run_mode == 'debug':\n",
    "            print(shingle_set)\n",
    "        \n",
    "        return shingle_set_hash\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSets:\n",
    "    # not necessarily need this\n",
    "    def Create_Global_Set(self, set_list):\n",
    "        set_global = []\n",
    "        for set_x in set_list:\n",
    "            set_global += set_x\n",
    "        \n",
    "        set_global_unique = np.unique(set_global)\n",
    "        \n",
    "        return set_global_unique\n",
    "    \n",
    "    def Compare_2_Sets(self, set1, set2):\n",
    "        common_shingles = 0\n",
    "     \n",
    "        set_union = set1 + set2\n",
    "        set_union_len = len(set_union)\n",
    "        set_unique = np.unique(set_union)\n",
    "        set_unique_size = set_unique.size\n",
    "        \n",
    "        # (set_union_len - set_unique_size) gives the intersection = duplicates\n",
    "        Jaccard_similarity = (set_union_len - set_unique_size)/set_unique_size\n",
    "        \n",
    "        return Jaccard_similarity\n",
    "    \n",
    "    def Compare_2_Sets_with_GlobalSet(self, set1, set2):\n",
    "        common_shingles = 0\n",
    "      \n",
    "        set_union = set1 + set2\n",
    "        set_union_len = len(set_union)\n",
    "        set_unique = np.unique(set_union)\n",
    "        set_unique_size = set_unique.size\n",
    "        \n",
    "        # (set_union_len - set_unique_size) gives the intersection = duplicates\n",
    "        Jaccard_similarity = (set_union_len - set_unique_size)/set_unique_size\n",
    "        \n",
    "        return Jaccard_similarity, set_unique\n",
    "    \n",
    "    def Compare_Sets_Global(self, set1, set2, set_global_unique):      \n",
    "        set_union = set1 + set2\n",
    "        set_union_len = len(set_union)\n",
    "        set_unique = np.unique(set_union)\n",
    "        set_unique_size = set_unique.size\n",
    "     \n",
    "        # create 0/1 set\n",
    "        set_1_global = [1 if s in set1 else 0 for s in set_global_unique]\n",
    "        set_2_global = [1 if s in set2 else 0 for s in set_global_unique]\n",
    "        \n",
    "        # (set_union_len - set_unique_size) gives the intersection = duplicates\n",
    "        Jaccard_similarity = (set_union_len - set_unique_size)/set_unique_size\n",
    "        \n",
    "        return Jaccard_similarity, set_1_global, set_2_global\n",
    "    \n",
    "    def Generate_Sets_Global_01(self, set_list, set_global_unique):      \n",
    "        set_01 = []\n",
    "        # for each set:\n",
    "        # for each shingle in the given set (document), check the global set.\n",
    "        # Then create a new set in the same length of the global set by setting 0 if the global shingle is not in\n",
    "        # the given set, or by setting 1 if the global shingle is in the given set\n",
    "        # finally we will have #docs (12 in our case) of sets with 0/1 values each having the same length\n",
    "        # with the global shingle set\n",
    "        for i, set1 in enumerate(set_list):\n",
    "            print(\">>>> Generating 0/1 for set#{}... time: {}\".format(i, datetime.datetime.now()))\n",
    "            set_1_01 = [1 if s in set1 else 0 for s in set_global_unique]\n",
    "            set_01.append(set_1_01)\n",
    "        \n",
    "        return set_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinHashing:        \n",
    "    def Generate_Hash_Functions(self, len_set_global_unique, number_of_hashfunctions = 100):\n",
    "        # format: (ax + b) % c\n",
    "        # so we will need to pass (a, b, c) values\n",
    "        # c = len_set_global_unique = total number of unique shingles\n",
    "        # assuming that c will be equal to the len(set_global_unique), we need to pass (a, b)\n",
    "        hash_func_params = []\n",
    "        \n",
    "        total_funcs_generated = 0\n",
    "        while total_funcs_generated < number_of_hashfunctions:\n",
    "            a = random.randint(1, len_set_global_unique - 1)\n",
    "            b = random.randint(1, len_set_global_unique - 1)\n",
    "            if (a, b) not in hash_func_params:\n",
    "                hash_func_params += [(a, b)]\n",
    "                total_funcs_generated += 1\n",
    "        \n",
    "        return hash_func_params   \n",
    "        \n",
    "    # creates a signature for a set/document = set_1_global using hash functions with the provided params\n",
    "    def CreateMinHash_Signature(self, set_1_global, hash_func_params):\n",
    "        # inifinity for us is equal to any number greater than or equal to \"len_set_global_unique\"\n",
    "        # Because we will do a min(x ,y) comparison and the function value will be moded to the \"len_set_global_unique\"\n",
    "        # \"len_set_global_unique\" can be chosen as the max value/infinity for our application\n",
    "        len_set_global_unique = len(set_1_global)\n",
    "        # we create an array with the length of the global set, and set the values of each item to the infinity\n",
    "        # (infinity == len_set_global_unique) in our case\n",
    "        set_1_signature = np.full(len(hash_func_params), len_set_global_unique)\n",
    "        \n",
    "        # here, we are handling 1 document only\n",
    "        # for each row (shingle value) in the given set, we calculate the hash_function result\n",
    "        # if the value is not 0 (in other words if the value == 1)\n",
    "        # then we compare it with the existing value in the signature. \n",
    "        # If the new value is smaller than the previous value, we update the signature\n",
    "        for row in range(len_set_global_unique):\n",
    "            if set_1_global[row] != 0:\n",
    "                for h, params in enumerate(hash_func_params):\n",
    "                    hash_value = (row*params[0] + params[1]) % len_set_global_unique\n",
    "                    set_1_signature[h] = min(set_1_signature[h], hash_value)\n",
    "        \n",
    "        return set_1_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSignatures:\n",
    "    def Find_Similarity(self, set_1_signature, set_2_signature):\n",
    "        count_similar = 0\n",
    "        len_set_1_signature = len(set_1_signature)\n",
    "\n",
    "        for i in range(len_set_1_signature):\n",
    "            if set_1_signature[i] == set_2_signature[i]:\n",
    "                count_similar += 1\n",
    "\n",
    "        sim_s1_s2 = count_similar/len_set_1_signature\n",
    "\n",
    "        return sim_s1_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_File_2String(file_path):\n",
    "    file_read = open(file_path, \"r\")\n",
    "    file_context = file_read.read()\n",
    "    file_read.close()\n",
    "    return file_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> File data_1/Movies_5.11.2020_18.54.20_deleted_MORE_words.txt, creating shingles... time: 2020-11-09 17:46:04.772954\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.19.txt, creating shingles... time: 2020-11-09 17:46:04.788010\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.18.txt, creating shingles... time: 2020-11-09 17:46:04.963153\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.23.txt, creating shingles... time: 2020-11-09 17:46:05.048573\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.22.txt, creating shingles... time: 2020-11-09 17:46:05.267275\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.20.txt, creating shingles... time: 2020-11-09 17:46:05.297802\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.21.txt, creating shingles... time: 2020-11-09 17:46:05.312274\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.20_deleted_some_words.txt, creating shingles... time: 2020-11-09 17:46:05.441541\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.16.txt, creating shingles... time: 2020-11-09 17:46:05.462353\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.17.txt, creating shingles... time: 2020-11-09 17:46:05.497085\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.15.txt, creating shingles... time: 2020-11-09 17:46:05.820066\n",
      ">>>> File data_1/Movies_5.11.2020_18.54.14.txt, creating shingles... time: 2020-11-09 17:46:06.029858\n",
      ">>>> Generating the Global Set (unique shingles)... time: 2020-11-09 17:46:06.514924\n",
      ">>>> n_hash_functions: 100, c: 43936\n",
      ">>>> Generating 0/1 Sets for all docs... time: 2020-11-09 17:46:06.522578\n",
      ">>>> Generating 0/1 for set#0... time: 2020-11-09 17:46:06.522632\n",
      ">>>> Generating 0/1 for set#1... time: 2020-11-09 17:46:14.356911\n",
      ">>>> Generating 0/1 for set#2... time: 2020-11-09 17:46:40.244988\n",
      ">>>> Generating 0/1 for set#3... time: 2020-11-09 17:46:58.627199\n",
      ">>>> Generating 0/1 for set#4... time: 2020-11-09 17:47:28.248079\n",
      ">>>> Generating 0/1 for set#5... time: 2020-11-09 17:47:39.075441\n",
      ">>>> Generating 0/1 for set#6... time: 2020-11-09 17:47:46.538450\n",
      ">>>> Generating 0/1 for set#7... time: 2020-11-09 17:48:10.449231\n",
      ">>>> Generating 0/1 for set#8... time: 2020-11-09 17:48:17.852477\n",
      ">>>> Generating 0/1 for set#9... time: 2020-11-09 17:48:29.209621\n",
      ">>>> Generating 0/1 for set#10... time: 2020-11-09 17:49:04.839699\n",
      ">>>> Generating 0/1 for set#11... time: 2020-11-09 17:49:34.686692\n",
      ">>>> Generating Hash Functions to use... time: 2020-11-09 17:50:18.188782\n",
      ">>>> Creating the signatures per doc... time: 2020-11-09 17:50:18.189363\n"
     ]
    }
   ],
   "source": [
    "# we mention the directory where we store the docs to be compared\n",
    "data_directory = 'data_1/'\n",
    "document_list = os.listdir(data_directory)\n",
    "document_list_shingle_set = []\n",
    "\n",
    "shingle_obj = Shingling()\n",
    "cs = CompareSets()\n",
    "mh = MinHashing()\n",
    "comp_sig = CompareSignatures()\n",
    "n_hash_functions = 100\n",
    "\n",
    "for doc in document_list:\n",
    "    file_path = data_directory + doc    \n",
    "    str_doc = Read_File_2String(file_path)\n",
    "    print(\">>>> File {}, creating shingles... time: {}\".format(file_path, datetime.datetime.now()))\n",
    "    shingle_doc = shingle_obj.Compute_Shingle_Set_string(str_doc, k_shingles = 10)\n",
    "    document_list_shingle_set.append(shingle_doc)\n",
    "\n",
    "print(\">>>> Generating the Global Set (unique shingles)... time: {}\".format(datetime.datetime.now()))\n",
    "unique_shingles_global = cs.Create_Global_Set(document_list_shingle_set)\n",
    "c = len(unique_shingles_global)  # len_set_global_unique\n",
    "\n",
    "print('>>>> n_hash_functions: {}, c: {}'.format(n_hash_functions, c))\n",
    "\n",
    "print(\">>>> Generating 0/1 Sets for all docs... time: {}\".format(datetime.datetime.now()))\n",
    "document_list_set_01 = cs.Generate_Sets_Global_01(document_list_shingle_set, unique_shingles_global)\n",
    "\n",
    "print(\">>>> Generating Hash Functions to use... time: {}\".format(datetime.datetime.now()))\n",
    "hash_func_params = mh.Generate_Hash_Functions(c, n_hash_functions)\n",
    "\n",
    "print(\">>>> Creating the signatures per doc... time: {}\".format(datetime.datetime.now()))\n",
    "signature_list = []\n",
    "for i, set_x in enumerate(document_list_set_01):\n",
    "    signature_list.append(mh.CreateMinHash_Signature(set_x, hash_func_params))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JACCARD: (d0, d5, 0.896): Movies_5.11.2020_18.54.20_deleted_MORE_words.txt similar to Movies_5.11.2020_18.54.20.txt by the ratio: 0.896\n",
      "SIG: (d0, d5, 0.89): Movies_5.11.2020_18.54.20_deleted_MORE_words.txt similar to Movies_5.11.2020_18.54.20.txt by the ratio: 0.89\n",
      "\n",
      "JACCARD: (d0, d7, 0.934): Movies_5.11.2020_18.54.20_deleted_MORE_words.txt similar to Movies_5.11.2020_18.54.20_deleted_some_words.txt by the ratio: 0.934\n",
      "SIG: (d0, d7, 0.94): Movies_5.11.2020_18.54.20_deleted_MORE_words.txt similar to Movies_5.11.2020_18.54.20_deleted_some_words.txt by the ratio: 0.94\n",
      "\n",
      "JACCARD: (d5, d7, 0.956): Movies_5.11.2020_18.54.20.txt similar to Movies_5.11.2020_18.54.20_deleted_some_words.txt by the ratio: 0.956\n",
      "SIG: (d5, d7, 0.95): Movies_5.11.2020_18.54.20.txt similar to Movies_5.11.2020_18.54.20_deleted_some_words.txt by the ratio: 0.95\n",
      "\n",
      "JACCARD SIMILARITY MATRIX:\n",
      "[[1.    0.03  0.043 0.021 0.045 0.896 0.035 0.934 0.033 0.018 0.017 0.013]\n",
      " [0.03  1.    0.032 0.02  0.024 0.03  0.035 0.03  0.019 0.023 0.017 0.013]\n",
      " [0.043 0.032 1.    0.02  0.029 0.043 0.041 0.043 0.025 0.025 0.016 0.018]\n",
      " [0.021 0.02  0.02  1.    0.025 0.021 0.022 0.021 0.017 0.016 0.018 0.015]\n",
      " [0.045 0.024 0.029 0.025 1.    0.045 0.03  0.045 0.029 0.018 0.02  0.014]\n",
      " [0.896 0.03  0.043 0.021 0.045 1.    0.037 0.956 0.033 0.02  0.017 0.013]\n",
      " [0.035 0.035 0.041 0.022 0.03  0.037 1.    0.035 0.02  0.024 0.019 0.015]\n",
      " [0.934 0.03  0.043 0.021 0.045 0.956 0.035 1.    0.033 0.018 0.017 0.013]\n",
      " [0.033 0.019 0.025 0.017 0.029 0.033 0.02  0.033 1.    0.026 0.03  0.014]\n",
      " [0.018 0.023 0.025 0.016 0.018 0.02  0.024 0.018 0.026 1.    0.035 0.016]\n",
      " [0.017 0.017 0.016 0.018 0.02  0.017 0.019 0.017 0.03  0.035 1.    0.015]\n",
      " [0.013 0.013 0.018 0.015 0.014 0.013 0.015 0.013 0.014 0.016 0.015 1.   ]]\n",
      "\n",
      "SIGNATURE SIMILARITY MATRIX:\n",
      "[[1.   0.11 0.12 0.12 0.08 0.89 0.11 0.94 0.09 0.09 0.11 0.1 ]\n",
      " [0.11 1.   0.11 0.15 0.1  0.12 0.12 0.11 0.11 0.14 0.13 0.16]\n",
      " [0.12 0.11 1.   0.14 0.07 0.12 0.15 0.12 0.11 0.12 0.13 0.13]\n",
      " [0.12 0.15 0.14 1.   0.09 0.12 0.15 0.12 0.11 0.14 0.16 0.12]\n",
      " [0.08 0.1  0.07 0.09 1.   0.08 0.11 0.08 0.09 0.11 0.08 0.1 ]\n",
      " [0.89 0.12 0.12 0.12 0.08 1.   0.11 0.95 0.09 0.09 0.11 0.1 ]\n",
      " [0.11 0.12 0.15 0.15 0.11 0.11 1.   0.11 0.11 0.12 0.12 0.1 ]\n",
      " [0.94 0.11 0.12 0.12 0.08 0.95 0.11 1.   0.09 0.09 0.11 0.1 ]\n",
      " [0.09 0.11 0.11 0.11 0.09 0.09 0.11 0.09 1.   0.08 0.11 0.11]\n",
      " [0.09 0.14 0.12 0.14 0.11 0.09 0.12 0.09 0.08 1.   0.17 0.15]\n",
      " [0.11 0.13 0.13 0.16 0.08 0.11 0.12 0.11 0.11 0.17 1.   0.16]\n",
      " [0.1  0.16 0.13 0.12 0.1  0.1  0.1  0.1  0.11 0.15 0.16 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# >>>>>> JACCARD SIMILARITY MATRIX vs \n",
    "# >>>>>> SIGNATURE SIMILARITY MATRIX\n",
    "#cs = CompareSets()\n",
    "n_document = len(document_list)\n",
    "Matrix_Jaccard_similarity = np.zeros((n_document, n_document))\n",
    "Matrix_SIG_similarity = np.zeros((n_document, n_document))\n",
    "\n",
    "for d1 in range(n_document):\n",
    "    for d2 in range(n_document):\n",
    "        # since the similarity will be symmetric, we don't need to calculate it twice\n",
    "        if d1 <= d2:\n",
    "            js = cs.Compare_2_Sets(document_list_shingle_set[d1], document_list_shingle_set[d2])\n",
    "            Matrix_Jaccard_similarity[d1][d2] = round(js, 3)\n",
    "            Matrix_Jaccard_similarity[d2][d1] = Matrix_Jaccard_similarity[d1][d2]\n",
    "            Matrix_SIG_similarity[d1][d2] = comp_sig.Find_Similarity(signature_list[d1], signature_list[d2])\n",
    "            Matrix_SIG_similarity[d2][d1] = Matrix_SIG_similarity[d1][d2]\n",
    "            \n",
    "            if Matrix_Jaccard_similarity[d1][d2] >= 0.8 and d1 != d2:\n",
    "                print(\"\\nJACCARD: (d{}, d{}, {}): {} similar to {} by the ratio: {}\".format(d1, d2, Matrix_Jaccard_similarity[d1][d2], document_list[d1], document_list[d2], Matrix_Jaccard_similarity[d1][d2]))\n",
    "            if Matrix_SIG_similarity[d1][d2] >= 0.8 and d1 != d2:\n",
    "                print(\"SIG: (d{}, d{}, {}): {} similar to {} by the ratio: {}\".format(d1, d2, Matrix_SIG_similarity[d1][d2], document_list[d1], document_list[d2], Matrix_SIG_similarity[d1][d2]))\n",
    "        \n",
    "print(\"\\nJACCARD SIMILARITY MATRIX:\\n{}\".format(Matrix_Jaccard_similarity))\n",
    "print(\"\\nSIGNATURE SIMILARITY MATRIX:\\n{}\".format(Matrix_SIG_similarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSH - Locality Sensitive Hashing\n",
    "band_buckets = []\n",
    "n_band = 20\n",
    "r = 5\n",
    "#hash_temp = hash(str(list(np.zeros(100, int))))\n",
    "\n",
    "for b in range(n_band):\n",
    "    band_bucket_id = []\n",
    "    band_bucket_docid = []\n",
    "    for i, sig in enumerate(signature_list):\n",
    "        hash_temp = hash(str(signature_list[i][b:b+r]))\n",
    "        if hash_temp not in band_bucket_id:\n",
    "            band_bucket_id.append(hash_temp)\n",
    "            band_bucket_docid.append([i])\n",
    "        else:\n",
    "            bucket_id = band_bucket_id.index(hash_temp)\n",
    "            band_bucket_docid[bucket_id].append(i)\n",
    "    \n",
    "    band_buckets.append([b, band_bucket_docid])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [[0], [1], [2], [3], [4], [5, 7], [6], [8], [9], [10], [11]]],\n",
       " [1, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [2, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [3, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [4, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [5, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [6, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [7, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [8, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [9, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [10, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [11, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [12, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [13, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [14, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [15, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [16, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [17, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [18, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]],\n",
       " [19, [[0, 5, 7], [1], [2], [3], [4], [6], [8], [9], [10], [11]]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d0: Movies_5.11.2020_18.54.20_deleted_MORE_words.txt has 1808 characters\n",
      "d1: Movies_5.11.2020_18.54.19.txt has 6053 characters\n",
      "d2: Movies_5.11.2020_18.54.18.txt has 4300 characters\n",
      "d3: Movies_5.11.2020_18.54.23.txt has 6559 characters\n",
      "d4: Movies_5.11.2020_18.54.22.txt has 2591 characters\n",
      "d5: Movies_5.11.2020_18.54.20.txt has 2008 characters\n",
      "d6: Movies_5.11.2020_18.54.21.txt has 5298 characters\n",
      "d7: Movies_5.11.2020_18.54.20_deleted_some_words.txt has 1966 characters\n",
      "d8: Movies_5.11.2020_18.54.16.txt has 2796 characters\n",
      "d9: Movies_5.11.2020_18.54.17.txt has 8348 characters\n",
      "d10: Movies_5.11.2020_18.54.15.txt has 6717 characters\n",
      "d11: Movies_5.11.2020_18.54.14.txt has 10270 characters\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(document_list):\n",
    "    file_path = data_directory + doc\n",
    "    str_doc = Read_File_2String(file_path)\n",
    "    print(\"d{}: {} has {} characters\".format(i, doc, len(str_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(34161, 24848), (24050, 2570), (37414, 11497), (11174, 13063), (40991, 2879), (16805, 14204), (34324, 23839), (33270, 30855), (41558, 28641), (24005, 18379), (43550, 24780), (9954, 5634), (9820, 20219), (22965, 42033), (9507, 29827), (392, 21812), (12960, 28913), (11773, 6631), (37323, 35648), (33881, 31943), (41244, 27213), (185, 14624), (36122, 43387), (6883, 22232), (1714, 6332), (16890, 18790), (32709, 39189), (26177, 7397), (31619, 25409), (32477, 38090), (2853, 5560), (43575, 3955), (28262, 23840), (37653, 2088), (35143, 27116), (23996, 14345), (2861, 9462), (39461, 36042), (13216, 37193), (24257, 20586), (40854, 27638), (14274, 39960), (1189, 7171), (15305, 14494), (2222, 23414), (22284, 6834), (11022, 43655), (22910, 1004), (35568, 7520), (27885, 4626), (4459, 36789), (5403, 41326), (7922, 1738), (33890, 15659), (13289, 16208), (41850, 30511), (25298, 16944), (27088, 35576), (8224, 24936), (31747, 34641), (15313, 43576), (29059, 11996), (18625, 20990), (36667, 40322), (15024, 26993), (4541, 37833), (440, 10695), (6949, 14154), (35397, 28835), (20151, 395), (1421, 23809), (23130, 19934), (31162, 62), (18923, 18756), (14288, 30940), (951, 11143), (35462, 40766), (24181, 18118), (41931, 27305), (25091, 11479), (28884, 13325), (28627, 31925), (40144, 37584), (20802, 12945), (12755, 14445), (14811, 41456), (22533, 39582), (19275, 34183), (24234, 14000), (23904, 32902), (2759, 16895), (20453, 22113), (29763, 28436), (36722, 37639), (15727, 21323), (17981, 23960), (33414, 18638), (9766, 5003), (16571, 23962), (830, 28092)]\n"
     ]
    }
   ],
   "source": [
    "print(hash_func_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2, 13,  3,  1, 28, 11,  3,  1,  1, 10,  4,  7,  1,  4,  4, 17,\n",
       "        2,  4,  2, 13,  1, 11, 13, 18,  8, 18,  9,  3, 12,  6, 19,  4, 18,\n",
       "       33,  1,  4,  3,  9,  8,  8,  2,  7,  7, 14,  2,  1,  4,  0,  0, 31,\n",
       "        1,  0, 29, 16,  7,  4,  8,  8, 13,  6, 15,  9,  9,  1,  4, 15,  0,\n",
       "        2,  1, 15,  2, 12,  5, 12,  4, 12,  5,  5,  1,  5,  2,  0, 11,  7,\n",
       "        8, 29,  2,  2,  6,  2,  0,  5,  9, 10,  8, 12,  1,  0, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  7,  14,  43,  29,   0,  69,  23,  33,   5,  13,  14,  66,  43,\n",
       "         22,  94,  44,  17,  11,  12,   8,  21,   9,   7,  45,  34,  50,\n",
       "         37,   9,  49,   1,  38,  38, 100,  21,  54,  49,  41,  23,   9,\n",
       "         26,  22,  24,   1,  26,  94,  30,  51,   4,   0,  25,  32,  69,\n",
       "        124,   5,  27,  59,  36,   8,   8,  59,  47,  14,  31,   5,   1,\n",
       "         34,  79,  98,  40,  72,  79,  10,   2,  22,  12, 169,   0,   2,\n",
       "         28,  14,  37,  23,   0,   3,  19,  29,  38,  51,  20,   6,  47,\n",
       "        100,  33,  39,  11,  32,  10, 101,  68,  72]),\n",
       " array([ 3,  2, 13,  3,  1, 28, 11,  3,  1,  1, 10,  4,  7,  1,  4,  4, 17,\n",
       "         2,  4,  2, 13,  1, 11, 13, 18,  8, 18,  9,  3, 12,  6, 19,  4, 18,\n",
       "        33,  1,  4,  3,  9,  8,  8,  2,  7,  7, 14,  2,  1,  4,  0,  0, 31,\n",
       "         1,  0, 29, 16,  7,  4,  8,  8, 13,  6, 15,  9,  9,  1,  4, 15,  0,\n",
       "         2,  1, 15,  2, 12,  5, 12,  4, 12,  5,  5,  1,  5,  2,  0, 11,  7,\n",
       "         8, 29,  2,  2,  6,  2,  0,  5,  9, 10,  8, 12,  1,  0, 10]),\n",
       " array([23,  0, 13,  1, 32,  6,  3, 13,  3, 13, 12,  2, 31, 30,  2, 12, 17,\n",
       "         8,  7, 17,  5, 14,  9, 25,  4,  0,  5,  8, 34, 11, 28,  4, 16, 28,\n",
       "         1,  5, 21, 11,  9, 21, 12, 14, 10, 26, 12,  2,  3,  0,  0,  4,  3,\n",
       "         8,  8, 33, 13,  3,  2,  8,  8,  2,  0,  2, 17,  3,  1, 11,  7,  3,\n",
       "         0, 11, 24, 14,  4,  6, 12,  2,  2,  9,  6,  9,  1, 23,  0, 53,  5,\n",
       "        10, 17,  4,  6,  6,  1, 10,  7, 29, 24,  5,  8, 15,  6,  6]),\n",
       " array([ 5,  6,  1,  3,  6,  3,  3, 15,  9,  8,  8,  6,  7,  5,  5,  4, 17,\n",
       "         7, 15,  8,  1, 12, 11, 27,  0, 20, 10,  2,  7,  6,  9,  2,  8,  7,\n",
       "         4,  5, 23,  2,  9, 24,  2,  2,  6,  3,  2, 18,  3, 10,  0,  6,  7,\n",
       "         4, 14,  5, 14,  5, 20,  8,  8, 25,  5, 13,  6, 17,  1,  0,  7,  2,\n",
       "         5,  5,  1, 12,  2,  3, 12,  9,  4,  7,  2,  4,  1,  5,  0, 31,  4,\n",
       "        14,  2,  1,  2,  6,  5,  3,  9,  7,  7,  4,  0,  5,  4, 30]),\n",
       " array([ 32,   4,  17,  57,   2,   5,  19,   1,  13,   9,  28,  24,  63,\n",
       "         11,  20,   4,  17,  14,  10,  48,   1,  16,  27,  19,   2,  16,\n",
       "          6,  33,   4,  10,  10,  20,   2,   2,  78,  13, 116,  26,   9,\n",
       "         29,   0,   6,  29,  14,  10,  30,  53,   6,   0,  44,  44,  12,\n",
       "         12,   7,   9,  13,   6,  88,   8,   0,  45, 108,  29,   8,   1,\n",
       "        106,  39,  36,  43,   8,  12,   0,  28,   8,  28,  77,  10,   4,\n",
       "         18,  12,   5,   6,   0,   5,  14,  19,   9,   0,  24,   6,  60,\n",
       "          9,   1,  11,   2,  18,  30,   1,  21,   8]),\n",
       " array([  0,  14,  43,  29,   0,  69,  23,  33,   5,  13,  14,  66,  43,\n",
       "         22,  94,  44,  17,  11,  12,   8,  21,   9,   7,  45,  34,  54,\n",
       "         37,   9,  49,   1,  38,  38, 100,  21,  54,  49,  41,  63,   9,\n",
       "         26,  22,  24,   1,  26,  14,  30,   5,   4,   0,  25,   6,  69,\n",
       "        104,   5,  27,  59,  36,   8,   8,  59,  47,  14,  31,   5,   1,\n",
       "         58,  79,  98,  40,  72,  29,  10,   2,   0,  12, 169,   0,   2,\n",
       "         28,  14,  37,  23,   0,   3,  19,  29,  38,  51,  20,   6,  47,\n",
       "        100,  33,  39,  11,  32,  10, 101,  57,  72]),\n",
       " array([ 9,  4, 35,  7, 25,  7,  3,  7,  5, 10,  0,  0,  7, 14,  2, 12, 17,\n",
       "         5,  0, 10,  5,  2,  5,  7, 16,  2,  1,  0, 19,  2, 11, 29, 10, 17,\n",
       "        12, 25,  0, 16,  9,  2,  0,  0, 23, 20,  8, 30, 11, 12,  0,  7, 15,\n",
       "        21,  0, 19, 20,  1,  2,  8,  8,  3,  3,  3, 11,  7,  1,  8, 47,  1,\n",
       "        20, 14,  5, 18,  0,  7, 12,  0,  8,  6, 17, 18,  1, 11,  0,  1,  6,\n",
       "         7,  9, 14,  2,  6, 13,  2,  2, 17,  3, 21,  0,  5, 14, 14]),\n",
       " array([  0,  14,  43,  29,   0,  69,  23,  33,   5,  13,  14,  66,  43,\n",
       "         22,  94,  44,  17,  11,  12,   8,  21,   9,   7,  45,  34,  54,\n",
       "         37,   9,  49,   1,  38,  38, 100,  21,  54,  49,  41,  23,   9,\n",
       "         26,  22,  24,   1,  26,  94,  30,   5,   4,   0,  25,   6,  69,\n",
       "        124,   5,  27,  59,  36,   8,   8,  59,  47,  14,  31,   5,   1,\n",
       "         58,  79,  98,  40,  72,  79,  10,   2,   0,  12, 169,   0,   2,\n",
       "         28,  14,  37,  23,   0,   3,  19,  29,  38,  51,  20,   6,  47,\n",
       "        100,  33,  39,  11,  32,  10, 101,  68,  72]),\n",
       " array([14, 18,  9, 41, 15, 13, 19,  7, 23, 21, 10, 22, 31, 32, 71, 12, 17,\n",
       "        53,  3,  5, 45,  3, 59, 39, 12, 14,  7, 27,  2,  8, 36,  3, 12,  6,\n",
       "         6, 13,  3,  9,  9, 11,  8, 28, 74,  8, 32, 14, 43, 44,  0,  1, 26,\n",
       "         6, 20,  5,  2,  5, 22, 72,  8, 31,  1,  7, 36,  0,  1, 16, 55,  4,\n",
       "        21,  0,  4, 64,  4, 14, 12, 13, 26,  0, 23, 10,  9,  4,  0, 17, 67,\n",
       "        21,  6,  7,  0,  6, 24, 17,  4,  9,  1,  4,  2,  9, 44, 14]),\n",
       " array([ 4, 12,  3,  5,  5,  2,  7,  9,  3,  5,  0, 20, 11, 17,  7,  4, 17,\n",
       "         1,  2,  7,  1,  0,  1,  2, 22,  0,  2,  1,  9,  0,  3,  2, 10,  1,\n",
       "         0,  1,  5,  1,  9,  1,  2, 10,  9,  4,  0,  2,  1,  2,  0,  2, 11,\n",
       "        10,  4,  1,  1,  9,  4,  8,  8,  5,  9,  1,  5,  4,  1,  1, 15, 10,\n",
       "         3,  5,  0,  0, 32,  9, 28,  1,  0,  8,  1,  6,  1,  1,  0,  1,  0,\n",
       "         0,  5,  3, 10,  6,  7, 13,  0,  3,  4,  3,  2,  7, 11,  4]),\n",
       " array([ 1,  0,  1,  1,  8,  1,  7, 25,  9,  7,  4,  2,  3,  0,  3,  4, 17,\n",
       "         0,  5,  0,  9, 10,  3, 12,  6,  4,  3,  7,  1,  3,  2,  0,  0,  0,\n",
       "        11,  1, 31, 10,  9,  0,  2,  0,  4,  6,  0,  6, 31,  0,  0, 10,  0,\n",
       "        29,  4,  5,  2,  1, 24,  8,  8,  4,  9,  0,  0,  1,  1,  7, 15,  8,\n",
       "        11,  5,  2,  6,  0,  1, 12,  3,  4,  1,  3,  3, 37,  0,  0, 11, 12,\n",
       "         5,  4,  8,  0,  6,  3, 31, 11,  1,  0,  0,  4,  3,  1,  2]),\n",
       " array([ 8,  8,  9,  9,  3,  0,  7,  1,  1,  0,  2, 10,  3,  2,  0,  4, 17,\n",
       "        18,  1,  3,  9,  5,  3,  0,  0,  6,  0,  4,  0,  4,  0,  6, 12,  4,\n",
       "         2,  1,  2,  0,  9,  3,  6,  4,  0,  0,  4,  2,  7,  2,  0,  3,  2,\n",
       "         0,  0,  1,  0,  3,  0,  8,  8,  5,  2,  4,  1, 30,  1,  3,  7,  5,\n",
       "         1,  2,  6,  2,  6,  4, 12,  8,  2, 24,  0,  0,  5,  3,  0,  3,  1,\n",
       "         1,  0, 16,  8,  6,  0,  1,  3,  1,  6,  1, 24,  9,  2,  0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
